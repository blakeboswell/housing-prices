{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d1da42",
   "metadata": {},
   "source": [
    "# Inital Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfc5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc199c",
   "metadata": {},
   "source": [
    "## Read in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/data/housing_prices/input/train.csv.gz'\n",
    "test_path  = '/data/housing_prices/input/test.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42862b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col, id_col = 'SalePrice', 'Id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (pd.read_csv(train_path)   \n",
    "    .dropna(subset=[y_col])\n",
    "    .set_index(id_col)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x_train[y_col]\n",
    "x_train = x_train.drop(y_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b05c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37feb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy = x_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fb64e",
   "metadata": {},
   "source": [
    "## Split Features by dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afef51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flt_cols = all_cols[x_train.dtypes.astype(str).isin(['float64'])].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e88fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = all_cols[x_train.dtypes.astype(str).isin(['int64'])].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a156de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = all_cols[x_train.dtypes.astype(str).isin(['object'])].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463eefa",
   "metadata": {},
   "source": [
    "## Normalize Category Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_levels(X: pd.DataFrame, cat_cols: list) -> pd.DataFrame:    \n",
    "    def normalize(x):\n",
    "        if x.name in cat_cols:\n",
    "            return x.str.lower().replace('\\s+', '', regex=True)\n",
    "        return x\n",
    "    return X.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtnorm_step1 = FunctionTransformer(normalize_levels, kw_args={'cat_cols': cat_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4103c",
   "metadata": {},
   "source": [
    "Replace labels that appear in the train data but don't match the data dictionary. The [below](#Encode-Categorical-Data) were discovered below when attempting to encode the categorical fields with the information provided in the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a843db",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_lvls = {\n",
    "    'Exterior2nd': {'wdshng': 'wdshing', 'cmentbd': 'cemntbd', 'brkcmn': 'brkcomm'},\n",
    "    'MSZoning': {'c(all)': 'c'},\n",
    "    'BldgType': {'duplex': 'duplx', 'twnhs': 'twnhsi'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f27e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_levels(X: pd.DataFrame, replace_map: dict) -> pd.DataFrame:\n",
    "    def replace(x):\n",
    "        if x.name in replace_map:\n",
    "            return x.replace(replace_map[x.name])\n",
    "        return x\n",
    "    return X.apply(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101cc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtnorm_step2 = FunctionTransformer(replace_levels, kw_args={'replace_map': replace_lvls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtnorm_trans = Pipeline(steps=[\n",
    "    ('step1', txtnorm_step1), ('step2', txtnorm_step2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = txtnorm_trans.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a138e6",
   "metadata": {},
   "source": [
    "## Missing Values Stage I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642b90d",
   "metadata": {},
   "source": [
    "From the data dictionary, NA is often used to represent a \"missing\" category rather than unknown information.  There are also instances where if one column is NA then other columns should simultaneousy be NA.  Therefore, data imputationwill require a two-staged strategy:\n",
    "\n",
    " - replace NA-as-a-level values with a label\n",
    " - fill actual missing NA values\n",
    " \n",
    "\n",
    "Here we focus on the first stage.  Each set among the following should have consistent missing categorization. \n",
    "\n",
    "```\n",
    "(\"MasVnrType\", \"MasVbrArea\")\n",
    "(\"MiscFeature\", \"MiscVal\")\n",
    "(\"PoolQC\", \"PoolArea\")\n",
    "(\"Fireplaces\", \"FireplaceQu\") \n",
    "(\"GarageType\", \"GarageYrBlt\", \"GarageFinish\", \"GarageQual\", \"GarageCond\")\n",
    "(\"BsmtCond\", \"BsmtQual\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinSF1\", \"BsmtFinType2\", \"BsmtFinSF2\", \"BsmtFullBath\", \"BsmtHalfBath\", \"BsmtUnfSF\")\n",
    "```\n",
    "\n",
    "We create the below data structure to hold the relationship:\n",
    "\n",
    "```\n",
    "[\"column that determines NA Status\", {\"column that depends on status\": \"NA label\"}]\n",
    "```\n",
    "This setup allows for multiple columns to depend on one column and for each dependent column to have it's own replacement value.  It also allows for a columns that are only dependent on their own values (for example see `Alley`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b02577",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_map = [\n",
    "    ['Alley', {\n",
    "        'Alley': 'na'\n",
    "    }],\n",
    "    ['BsmtCond', {\n",
    "        'BsmtQual': 'na'\n",
    "        , 'BsmtExposure': 'na'\n",
    "        , 'BsmtFinType1': 'na'\n",
    "        , 'BsmtFinSF1': 0.0\n",
    "        , 'BsmtFinType2': 'na'\n",
    "        , 'BsmtFinSF2':  0.0\n",
    "        , 'BsmtFullBath': 0.0\n",
    "        , 'BsmtHalfBath': 0.0\n",
    "        , 'BsmtUnfSF': 0.0\n",
    "    }],\n",
    "    ['BsmtCond', {\n",
    "        'BsmtCond': 'na'\n",
    "    }],\n",
    "    ['GarageType', {\n",
    "        'GarageYrBlt': -1.0\n",
    "        , 'GarageFinish': 'na' \n",
    "        , 'GarageCars': 0.0\n",
    "        , 'GarageArea': 0.0\n",
    "        , 'GarageQual': 'na'\n",
    "        , 'GarageCond': 'na'\n",
    "        , 'GarageYrBlt': -1.0\n",
    "    }], \n",
    "    ['GarageType', {\n",
    "        'GarageType': 'na'\n",
    "    }],\n",
    "    ['Fence', {\n",
    "        'Fence': 'na'\n",
    "    }],\n",
    "    ['FireplaceQu', {\n",
    "        'Fireplaces': 'na'\n",
    "    }],\n",
    "    ['FireplaceQu', {\n",
    "        'FireplaceQu': 'na'\n",
    "    }],\n",
    "    ['MasVnrType', {\n",
    "        'MasVnrArea': 0.0\n",
    "    }],\n",
    "    ['MasVnrType', {\n",
    "        'MasVnrType': 'none'\n",
    "    }],\n",
    "    ['MiscFeature', {\n",
    "        'MiscVal': 0.0\n",
    "    }],\n",
    "    ['MiscFeature', {\n",
    "        'MiscFeature': 'na'\n",
    "    }],\n",
    "    ['PoolQC', {\n",
    "        'PoolQC': 'na'\n",
    "    }]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922fed6",
   "metadata": {},
   "source": [
    "The following function is compatible with sklearn pipeline and was written to accomodate the above data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea159ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_fill(X: pd.DataFrame, impute_map: dict) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    Xcopy = X.copy()\n",
    "    def fill_column(x, na_loc, fill_val):\n",
    "        try:\n",
    "            return np.where(na_loc & x.isna(), fill_val[x.name], x)\n",
    "        except KeyError:\n",
    "            return x\n",
    "    \n",
    "    for k, v in impute_map:\n",
    "        Xcopy = Xcopy.apply(fill_column, na_loc=Xcopy[k].isna(), fill_val=v)\n",
    "\n",
    "    return Xcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_step1 = FunctionTransformer(conditional_fill, kw_args={'impute_map': impute_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a24fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = impute_step1.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2064f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.isna().sum()[x_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73479efc",
   "metadata": {},
   "source": [
    "## Get Levels from Data Dictionary\n",
    "\n",
    "There are many oridinal fields in the data.  The data dictionary contains the ordinal values in a reasonable order that was derived from some process outside of the data. Let's make that something we can apply directly in data preprocessing in a pipeline.\n",
    "\n",
    "Some data is probably better off one-hot encoded so we won't need the ordinal information ... but just grabbing it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = '/data/housing_prices/input/data_description.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_feed(filepath: str):\n",
    "    \"\"\" combine lines into chunks and split chunks based\n",
    "        on appearance of ':' which indicates a new field\n",
    "        definition\n",
    "    \"\"\"\n",
    "    def detect_field(line):\n",
    "        tokens = line.split(' ')\n",
    "        return tokens[0].endswith(':')\n",
    "\n",
    "    group = []\n",
    "    for i, line in enumerate(open(filepath)):\n",
    "        line = line.strip()\n",
    "        if i > 0 and detect_field(line):\n",
    "            yield group\n",
    "            group = []\n",
    "        if line: \n",
    "            group.append(line)\n",
    "    yield group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e8c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_levels(record: str) -> dict:\n",
    "    \"\"\" create a dict of {'field': ['levels']} by \n",
    "        splitting on ':' for fields and '\\t' for levels\n",
    "    \"\"\"\n",
    "    def clean_lvl(line):\n",
    "        lvl = line.split('\\t')[0]\n",
    "        return lvl.strip().lower().replace(' ', '')\n",
    "    \n",
    "    if len(record) > 1:\n",
    "        cat, *lvls = record\n",
    "        return cat.split(':')[0], [clean_lvl(lvl) for lvl in lvls]\n",
    "    return record[0].split(':')[0], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = {\n",
    "    k: v for k, v in map(make_levels, list(record_feed(dictionary_path))) \n",
    "    if v and k in cat_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd68d5",
   "metadata": {},
   "source": [
    "## Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_encoder(X:pd.DataFrame, encodings: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x_train = X.copy()\n",
    "    for k, levels in encodings.items():\n",
    "        encoding = {lvl: i for i, lvl in enumerate(levels)}\n",
    "        x_train[k] = x_train[k].map(encoding)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_trans = FunctionTransformer(ordinal_encoder, kw_args={'encodings': levels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bfce33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train = ordinal_trans.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63716f",
   "metadata": {},
   "source": [
    "Make sure we didn't introduce any new NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d963432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.isna().sum()[x_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28be95",
   "metadata": {},
   "source": [
    "## Missing Values Stage II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bc6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.loc[x_train['BsmtExposure'].isna(), x_train.columns[x_train.columns.str.startswith('Bsmt')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.loc[x_train['BsmtFinType2'].isna(), x_train.columns[x_train.columns.str.startswith('Bsmt')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da1f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.loc[x_train['Electrical'].isna(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eafbb1",
   "metadata": {},
   "source": [
    "`BsmtExposure`, `BsmtFinType2` and `Electrical` seem like data entry errors.  `LotFrontage` is missing.  \n",
    "\n",
    "We can use keep thing simple and use `median` strategy for `LotFrontage` and `mode` for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce29a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_stage2 = ColumnTransformer([\n",
    "    ('median', SimpleImputer(strategy='median'), ['LotFrontage']),\n",
    "    ('mode', SimpleImputer(strategy='most_frequent'), ['BsmtExposure', 'BsmtFinType2', 'Electrical'])\n",
    "], remainder='passthrough', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035e9e7",
   "metadata": {},
   "source": [
    "## Operationalize\n",
    "\n",
    "\n",
    "Now that we've identified a baseline set of operations that are required before baseline modeling, let's organize into a single pipeline, test, and save artifacts necessary for replicating the process outside of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_trans = Pipeline(steps=[\n",
    "    ('txtnorm_trans', txtnorm_trans),\n",
    "    ('impute_step1', impute_step1),\n",
    "    ('ordinal_trans', ordinal_trans),\n",
    "    ('impute_stage2', impute_stage2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy = preprocessing_trans.fit_transform(x_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad462a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3056fc",
   "metadata": {},
   "source": [
    "## Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_meta = {\n",
    "    'flt_cols': flt_cols,\n",
    "    'int_cols': int_cols,\n",
    "    'cat_cols': cat_cols,\n",
    "    'replace_lvls': replace_lvls,\n",
    "    'impute_map': impute_map,\n",
    "    'levels': levels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee97698",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/housing_prices/input/feature_meta.json', 'w') as f:\n",
    "    json.dump(feature_meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json.load(open('/data/housing_prices/input/feature_meta.json'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
